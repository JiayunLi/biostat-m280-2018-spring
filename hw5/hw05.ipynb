{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 5\n",
    "\n",
    "**Due June 15 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. In [HW4](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html), we worked out a Newton's method. In this homework, we explore the MM and EM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Q1  \n",
    "\n",
    "In HW4, we have already shown that the log-likelihood of the Dirichlet-multinomial distribution is $L(\\alpha) = \n",
    "\\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]\n",
    "$, then we can have:  \n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "L(\\alpha) &= \n",
    "\\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)] \\\\ &= \n",
    "\\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\ln \\frac{\\Gamma(\\alpha_j + x_{ij})}{\\Gamma(\\alpha_j)} -  \\sum_{i=1}^n \\ln \\frac{\\Gamma(|\\alpha|+|\\mathbf{x}_i|)}{\\Gamma(|\\alpha|)} \\\\ &=\n",
    "\\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\ln[\\alpha_j(\\alpha_j+1)\\cdots(\\alpha_j+x_{ij}-1)] - \n",
    "\\sum_{i=1}^n \\ln[|\\alpha|(|\\alpha|+1)\\cdots(|\\alpha|+|\\mathbf{x}_i|-1)] \\\\ &=\n",
    "\\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Q2  \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\Gamma(|\\alpha|)} = \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,d\\mathbf{p}\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "We can take derivative of $\\alpha_i$ from both sides of the equation:  \n",
    "\n",
    "$$\n",
    "\\frac{\\Gamma(|\\alpha|)\\prod_{j=1, j \\neq i}^d \\Gamma(\\alpha_j)\\Gamma'(\\alpha_i)}{\\Gamma^2(|\\alpha|)} =\n",
    "\\int_{\\Delta_d}\\ln(p_i) \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\prod_{j=1}^d\\Gamma(\\alpha_j)}{\\Gamma(|\\alpha|)}(\\frac{\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} -\n",
    "\\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}) =\n",
    "\\int_{\\Delta_d}\\ln(p_i) \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,d\\mathbf{p}\n",
    "$$  \n",
    "\n",
    "Then we can have   \n",
    "\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Q3  \n",
    "\n",
    "If we treat the unobserved multinomial parameters as hidden variables, the density of complete data can be expressed as $f(\\mathbf{p},\\mathbf{x}| \\alpha)$  \n",
    "\n",
    "Then the conditional expectation:  \n",
    "\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = E_{\\mathbf{p}|x,\\alpha^{(t)}}[\\ln f(\\mathbf{p},\\mathbf{x}\\,|\\, \\alpha)\\,|\\,x, \\alpha^{(t)})]\n",
    "$$  \n",
    "\n",
    "Then we need to compute $\\ln f(\\mathbf{p},\\mathbf{x}\\,|\\, \\alpha)$. The density of Dirichlet-multinomial distribution is \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1},\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "where $|\\alpha|=\\sum_{j=1}^d \\alpha_j$.  \n",
    "\n",
    "$$\n",
    "f(x|\\mathbf{p}, \\alpha) = \\frac{\\Gamma(\\sum_{j=1}^d(x_j+1))}{\\prod_{j=1}^d\\Gamma(x_j+1)}\\prod_{j=1}^dp_j^{x_j}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "f(x,\\mathbf{p}\\,|\\,\\alpha) = \\frac{\\Gamma(\\sum_{j=1}^d(x_j+1))}{\\prod_{j=1}^d\\Gamma(x_j+1)}\n",
    "\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d\\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{x_j+\\alpha_j-1}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "f(x,\\mathbf{p}\\,|\\,\\alpha) = \\ln\\Gamma(\\sum_{j=1}^d(x_j+1)) + \\ln\\Gamma(|\\alpha|) -\n",
    "\\sum_{j=1}^d\\Gamma(x_j+1) - \\sum_{j=1}^d\\Gamma(\\alpha_j) + \n",
    "\\sum_{j=1}^d(\\alpha_j+x_j-1)\\ln p_j\n",
    "$$  \n",
    "\n",
    "The Q function can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q(\\alpha|\\alpha^{(t)}) &= \\sum_{i=1}^n\\sum_{j=1}^d(\\alpha_j+x_j-1)E(\\ln p_j) - \n",
    "n\\sum_{j=1}^d\\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{t} \\\\\n",
    "&= \\sum_{i=1}^n\\sum_{j=1}^d[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)](\\alpha_j+x_j-1)E(\\ln p_j)- \n",
    "n\\sum_{j=1}^d\\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{t} \\\\\n",
    "&= \\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)}\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "Since $\\alpha_j$ are coupled with each other, it could be difficult to maximize the Q function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Q4  \n",
    "\n",
    "In order to show it's a minorizing function to $L(\\alpha)$, we need to prove:  \n",
    "\n",
    "$$\n",
    " g(\\alpha|\\alpha^{(t)}) \\leq L(\\alpha) \\\\\n",
    " g(\\alpha^{(t)}|\\alpha^{(t)}) == L(\\alpha^{(t)})\n",
    "$$  \n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$  \n",
    "\n",
    "\n",
    "The $-\\ln(\\alpha_j) + k$ is a convex, non-increasing function. According to Jensen inequality, we can have:  \n",
    "\n",
    "$$\n",
    "-\\ln(\\alpha_j + k) \\leq -\\ln(\\alpha_j\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k} + k\\frac{k}{\\alpha_j^{(t)}+k})\n",
    "\\leq -\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\ln(\\alpha_j)\n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\ln(\\alpha_j) \\leq \\ln(\\alpha_j + k) \n",
    "$$  \n",
    "\n",
    "Since $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, we can take summation on both sides:  \n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j \\leq \n",
    "\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) \n",
    "$$\n",
    "\n",
    "According to the hyperplane supporting inequality $f(x) + f'(x)(y-x) \\leq f(y)$, we let $y=|\\alpha|$, $x = |\\alpha^{(t)}|$, we can have:  \n",
    "\n",
    "$$\n",
    "-\\frac{1}{|\\alpha^{(t)}| + k}|\\alpha| + c^{(t)} \\leq -\\ln(|\\alpha|+k)\n",
    "$$  \n",
    "\n",
    "Since $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, we can take summation at both sides:  \n",
    "\n",
    "$$\n",
    "- \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha|+ c^{(t)} \\leq - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)\n",
    "$$  \n",
    "\n",
    "The equality achieves when $\\alpha_j == \\alpha_j^{(t)}$  So we have shown that $g(\\alpha|\\alpha^{(t)})$ is the minorizing function to $L(\\alpha)$.  \n",
    "\n",
    "From the above, we know the minorizing function of $L(\\alpha)$ is $g(\\alpha|\\alpha^{(t)})$. In order to get the update for $\\alpha_j$, we can take the derivative of $g(\\alpha|\\alpha^{(t)})$ with respect to $\\alpha_j$ and get the $\\alpha_j^{(t+1)}$ by solving $\\frac{dg(\\alpha|\\alpha^{(t)})}{d\\alpha_j} = 0$:  \n",
    "\n",
    "$$\n",
    "- \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha_j^{(t)}|+k} + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\frac{1}{\\alpha_j}=0 \n",
    "$$  \n",
    "\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code from HW4: compute the log likelihood of Dirichlet-multinomial distribution\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    # your code here\n",
    "    sum_x = sum(x)\n",
    "    sum_α = sum(α)\n",
    "    L = lgamma(sum_x + 1) - sum(lgamma.(x + 1)) + \n",
    "        sum(lgamma.(x + α)) - sum(lgamma.(α)) - \n",
    "        lgamma(sum_α + sum_x) + lgamma(sum_α)\n",
    "    return L\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "    X::AbstractMatrix; \n",
    "    #α0::Vector = nothing,\n",
    "    maxiters::Int = 100, \n",
    "    tolfun = 1e-6\n",
    "    )\n",
    "    tot_n = size(X, 2)\n",
    "    tot_d = size(X, 1)\n",
    "    α0 = nothing\n",
    "    nonzero_indx = find(sum(X, 2) .!= 0)\n",
    "    # rule zero zeros\n",
    "    X = X[nonzero_indx, :]\n",
    "    # Compute X_i\n",
    "    X_rowsum = sum(X, 1)\n",
    "    n = size(X, 2)\n",
    "    d = size(X, 1)\n",
    "    # If the α is no initialized, use the moment estimator as a starting point\n",
    "    if α0 == nothing\n",
    "        p_hat = X ./ X_rowsum\n",
    "        v = sum(sum(p_hat.^2, 2) ./ sum(p_hat, 2))\n",
    "        α0 = sum(p_hat, 2) / n * (d - v) / (v - 1)\n",
    "    end\n",
    "    α0_sum = sum(α0)\n",
    "    # compute max x_ij - 1\n",
    "    max_ij_1 = maximum(X) - 1\n",
    "    # compute max |x_i| - 1\n",
    "    max_abs_1 = maximum(X_rowsum) - 1\n",
    "    # compute the S matrix\n",
    "    S = zeros(size(X, 1), max_ij_1 + 1)\n",
    "    ks = Array(range(0, Int(max_ij_1 + 1)))\n",
    "    # preallocate memory\n",
    "    aks = zeros(d, Int(max_ij_1 + 1))\n",
    "    for i = 1:Int(max_ij_1 + 1)\n",
    "        S[:, i] = sum(X .> (i - 1), 2)\n",
    "    end\n",
    "    kr = range(0, Int(max_abs_1 + 1))\n",
    "    R = zeros(max_abs_1 + 1)\n",
    "    for i = 1:Int(max_abs_1 + 1)\n",
    "        R[i] = sum(X_rowsum .> (i - 1))\n",
    "    end\n",
    "    loglold = sum(dirmult_logpdf(X, squeeze(α0, 2)))\n",
    "    # your code here\n",
    "    niter = 0\n",
    "    err = 100\n",
    "    # preallocate memory\n",
    "    nu = zeros(d, 1)\n",
    "    α1 = zeros(d, 1)\n",
    "    while (niter < maxiters) && (err > tolfun)\n",
    "        aks .= broadcast(+, α0, ks')\n",
    "        nu .= sum(S ./ aks, 2)\n",
    "        de = sum(R ./ (kr .+ α0_sum))\n",
    "        # compute a_(t + 1)\n",
    "        α1 .= (nu ./ de) .* α0\n",
    "        # compute new loglikelihood\n",
    "        logl = sum(dirmult_logpdf(X, squeeze(α1, 2)))\n",
    "        err = abs(loglold - logl) / (abs(loglold) - 1)\n",
    "        α0 .= α1\n",
    "        α0_sum = sum(α0)\n",
    "        loglold = logl\n",
    "        niter = niter + 1\n",
    "    end\n",
    "    # output\n",
    "    # compute logl, gradient, Hessian from final iterate\n",
    "    c = - sum(trigamma.(broadcast(+, α0_sum, X_rowsum)) - \n",
    "            trigamma(α0_sum))\n",
    "    H_tempt = - Diagonal(squeeze(sum(broadcast(-, \n",
    "        trigamma.(broadcast(+, X, α0)), \n",
    "        trigamma.(α0)), 2), 2)) - c * ones(d, d)\n",
    "    # compute observed information matrix (inverse of hessian)\n",
    "    obsinfo = zeros(tot_d, tot_d)\n",
    "    obsinfo[nonzero_indx, nonzero_indx] = - H_tempt\n",
    "    # compute gradients\n",
    "    dL = sum(broadcast(-, \n",
    "        digamma.(broadcast(+, X, α0)), digamma.(α0)), 2) - \n",
    "        sum(digamma.(broadcast(+, α0_sum, X_rowsum)) - \n",
    "        digamma(α0_sum))\n",
    "    ∇ = zeros(tot_d)\n",
    "    fill!(∇, sum(digamma.(broadcast(+, \n",
    "        α0_sum, X_rowsum)) - digamma(α0_sum)))\n",
    "    ∇[nonzero_indx] = dL\n",
    "    α = zeros(tot_d)\n",
    "    α[nonzero_indx] = α0\n",
    "    logl = loglold\n",
    "    return logl, niter, α, ∇, obsinfo\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_newton"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_newton(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using Newton's method.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `n`-by-`d` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: a `d` vector of starting point (optional). \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "* `maximum`: the log-likelihood at MLE.   \n",
    "* `estimate`: the MLE. \n",
    "* `gradient`: the gradient at MLE. \n",
    "* `hessian`: the Hessian at MLE. \n",
    "* `se`: a `d` vector of standard errors. \n",
    "* `iterations`: the number of iterations performed.\n",
    "\"\"\"\n",
    "# function dirmult_newton(X::Matrix; α0::Vector = nothing, \n",
    "            # maxiters::Int = 100, tolfun::Float64 = 1e-6)\n",
    "function dirmult_newton(X::Matrix; \n",
    "    maxiters::Int = 100, tolfun::Float64 = 1e-6)\n",
    "    \n",
    "    # set default starting point from Q7\n",
    "    tot_n = size(X, 2)\n",
    "    tot_d = size(X, 1)\n",
    "    α1 = nothing\n",
    "    α0 = nothing\n",
    "    nonzero_indx = find(sum(X, 2) .!= 0)\n",
    "    # rule zero zeros\n",
    "    X = X[nonzero_indx, :]\n",
    "    X_rowsum = sum(X, 1)\n",
    "    n = size(X, 2)\n",
    "    d = size(X, 1)\n",
    "    \n",
    "    # If the α is no initialized, use the moment estimator as a starting point\n",
    "    if α0 == nothing\n",
    "        p_hat = X ./ X_rowsum\n",
    "        v = sum(sum(p_hat.^2, 2) ./ sum(p_hat, 2))\n",
    "        α0 = sum(p_hat, 2) / n * (d - v) / (v - 1)\n",
    "    end\n",
    "    α0_sum = sum(α0)\n",
    "    # calculate the log likelihood of this data at the starting point\n",
    "    loglold = sum(dirmult_logpdf(X, squeeze(α0, 2)))\n",
    "    dL = zeros(d, 1)\n",
    "    ax = zeros(d, n)\n",
    "    axa = zeros(d, n)\n",
    "    inv_A = ones(d, 1)\n",
    "    ax_abs = zeros(1, n)\n",
    "    ax_tri = zeros(d, n)\n",
    "    direct = zeros(d, 1)\n",
    "    iterations = 0\n",
    "    # Newton loop\n",
    "    for iter in 1:maxiters\n",
    "        # evaluate gradient (score)\n",
    "        ax .= broadcast(+, X, α0)\n",
    "        axa .= broadcast(-, digamma.(ax), digamma.(α0))\n",
    "        ax_abs .= digamma.(broadcast(+, X_rowsum, α0_sum))\n",
    "        # compute the score\n",
    "        dL .= sum(axa, 2) - sum(ax_abs - digamma(α0_sum))\n",
    "        # compute Newton's direction\n",
    "        ax_tri = broadcast(-, trigamma.(ax), trigamma.(α0))\n",
    "        inv_A .= -1 ./ sum(ax_tri, 2)\n",
    "\n",
    "        # compute scaling factor c\n",
    "        c = - sum(trigamma.(broadcast(+, α0_sum, X_rowsum)) - \n",
    "            trigamma(α0_sum))\n",
    "        # the threshold to determin whether matrix is PD\n",
    "        thres = sum(inv_A)\n",
    "        if c < 1 / thres\n",
    "            c = c\n",
    "        else\n",
    "            c = 0.99 * 1 / thres\n",
    "        end\n",
    "        # computer the direction\n",
    "        direct .= inv_A .* dL + \n",
    "            (c / (1 - c * thres)) * inv_A * (inv_A' * dL)\n",
    "        s_temp = - 0.99 * α0 ./ direct\n",
    "        # ensure all steps are positive\n",
    "        s_temp[find(s_temp .< 0)] = 10\n",
    "        # get the minimum step\n",
    "        s = minimum(s_temp)\n",
    "        # line search loop\n",
    "        # for lsiter in 1:10\n",
    "        logl_new = loglold - 1\n",
    "        while logl_new < loglold\n",
    "            # step halving\n",
    "            s = s / 2\n",
    "            α1 = α0 + s * direct\n",
    "            logl_new = sum(dirmult_logpdf(X, squeeze(α1, 2)))\n",
    "        end\n",
    "        # compute the loglikelihood\n",
    "        logl = sum(dirmult_logpdf(X, squeeze(α1, 2)))\n",
    "        # check convergence criterion\n",
    "        if abs(logl - loglold) < tolfun * (abs(loglold) + 1)\n",
    "            loglold = logl\n",
    "            α0 = α1\n",
    "            α0_sum = sum(α0)\n",
    "            iterations = iterations + 1\n",
    "            break;\n",
    "        end\n",
    "        loglold = logl\n",
    "        α0 = α1\n",
    "        α0_sum = sum(α0)\n",
    "        iterations = iterations + 1\n",
    "    end\n",
    "    # compute logl, gradient, Hessian from final iterate\n",
    "    c = - sum(trigamma.(broadcast(+, α0_sum, X_rowsum)) - \n",
    "            trigamma(α0_sum))\n",
    "    H_tempt = - Diagonal(squeeze(sum(broadcast(-, \n",
    "        trigamma.(broadcast(+, X, α0)), \n",
    "        trigamma.(α0)), 2), 2)) - c * ones(d, d)\n",
    "    hessian = zeros(tot_d, tot_d)\n",
    "    hessian[nonzero_indx, nonzero_indx] = - H_tempt\n",
    "    # output\n",
    "    inv_A .= -1 ./ sum(broadcast(-, \n",
    "        trigamma.(broadcast(+, X, α0)), trigamma.(α0)), 2)\n",
    "    thres = sum(inv_A)\n",
    "    inv_H = Diagonal(Diagonal(squeeze(inv_A, 2)) + \n",
    "        c / (1 - c * thres) * (inv_A * inv_A'))[:]\n",
    "    inv_H = inv_H[find(inv_H .!= 0)]\n",
    "    se = zeros(tot_d)\n",
    "    se[nonzero_indx] = sqrt.(inv_H)\n",
    "    dL = sum(broadcast(-, \n",
    "        digamma.(broadcast(+, X, α0)), digamma.(α0)), 2) - \n",
    "        sum(digamma.(broadcast(+, α0_sum, X_rowsum)) - \n",
    "        digamma(α0_sum))\n",
    "    gradient = zeros(tot_d)\n",
    "    fill!(gradient, sum(digamma.(broadcast(+,\n",
    "        α0_sum, X_rowsum)) - digamma(α0_sum)))\n",
    "    gradient[nonzero_indx] = dL\n",
    "    estimate = zeros(tot_d)\n",
    "    estimate[nonzero_indx] = α0\n",
    "    maximum = loglold\n",
    "    return maximum, estimate, gradient, \n",
    "        hessian, se, iterations\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = Matrix{Float64}(CSV.read(\"optdigits.tra\", datarow = 1));\n",
    "X = digits[:, 1:64]';\n",
    "Y = digits[:, 65];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.052526897 seconds\n",
      "elapsed time: 0.043659785 seconds\n",
      "elapsed time: 0.077321317 seconds\n",
      "elapsed time: 0.064352807 seconds\n",
      "elapsed time: 0.09331962 seconds\n",
      "elapsed time: 0.057762274 seconds\n",
      "elapsed time: 0.055476407 seconds\n",
      "elapsed time: 0.040649566 seconds\n",
      "elapsed time: 0.070841303 seconds\n",
      "elapsed time: 0.040498305 seconds\n"
     ]
    }
   ],
   "source": [
    "# loglikelihood for newton method\n",
    "logl_nt = zeros(10)\n",
    "αs_nt = zeros(size(X, 1), 10)\n",
    "time_nt = zeros(10)\n",
    "niters_nt = zeros(10)\n",
    "for i = 1:10\n",
    "    tic()\n",
    "    mask_indx = find(Y .== (i - 1))\n",
    "    cur_X = X[:, mask_indx]\n",
    "    logl, α, _, _, \n",
    "        _, iters_nt = dirmult_newton(cur_X)\n",
    "    αs_nt[:, i] = α\n",
    "    logl_nt[i] = logl\n",
    "    time_nt[i] = toc()\n",
    "    niters_nt[i] = iters_nt\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 2.230367537 seconds\n",
      "elapsed time: 0.034154317 seconds\n",
      "elapsed time: 0.088893206 seconds\n",
      "elapsed time: 0.150598271 seconds\n",
      "elapsed time: 0.020802365 seconds\n",
      "elapsed time: 0.021913532 seconds\n",
      "elapsed time: 0.180918402 seconds\n",
      "elapsed time: 0.118963747 seconds\n",
      "elapsed time: 0.162813652 seconds\n",
      "elapsed time: 0.01743648 seconds\n"
     ]
    }
   ],
   "source": [
    "# loglikelihood for newton method\n",
    "logl_mm = zeros(10)\n",
    "αs_mm = zeros(size(X, 1), 10)\n",
    "time_mm = zeros(10)\n",
    "niters_mm = zeros(10)\n",
    "for i = 1:10\n",
    "    tic()\n",
    "    mask_indx = find(Y .== (i - 1))\n",
    "    cur_X = X[:, mask_indx]\n",
    "    logl, iters_mm, α, _, _ = dirmult_mm(cur_X)\n",
    "    αs_mm[:, i] = α\n",
    "    logl_mm[i] = logl\n",
    "    time_mm[i] = toc()\n",
    "    niters_mm[i] = iters_mm\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>loglikelihood_Newton</th><th>loglikelihood_MM</th><th>niters_Newton</th><th>niters_MM</th><th>time_Newton</th><th>time_MM</th></tr></thead><tbody><tr><th>1</th><td>-37358.4</td><td>-37364.4</td><td>7.0</td><td>100.0</td><td>0.0525269</td><td>2.23037</td></tr><tr><th>2</th><td>-42179.2</td><td>-42179.5</td><td>5.0</td><td>16.0</td><td>0.0436598</td><td>0.0341543</td></tr><tr><th>3</th><td>-39985.3</td><td>-39985.6</td><td>6.0</td><td>46.0</td><td>0.0773213</td><td>0.0888932</td></tr><tr><th>4</th><td>-40519.5</td><td>-40519.9</td><td>6.0</td><td>64.0</td><td>0.0643528</td><td>0.150598</td></tr><tr><th>5</th><td>-43488.8</td><td>-43488.9</td><td>7.0</td><td>7.0</td><td>0.0933196</td><td>0.0208024</td></tr><tr><th>6</th><td>-41191.3</td><td>-41191.5</td><td>6.0</td><td>7.0</td><td>0.0577623</td><td>0.0219135</td></tr><tr><th>7</th><td>-37702.5</td><td>-37703.0</td><td>6.0</td><td>71.0</td><td>0.0554764</td><td>0.180918</td></tr><tr><th>8</th><td>-40304.0</td><td>-40304.3</td><td>5.0</td><td>47.0</td><td>0.0406496</td><td>0.118964</td></tr><tr><th>9</th><td>-43130.8</td><td>-43131.3</td><td>6.0</td><td>52.0</td><td>0.0708413</td><td>0.162814</td></tr><tr><th>10</th><td>-43709.7</td><td>-43709.8</td><td>4.0</td><td>6.0</td><td>0.0404983</td><td>0.0174365</td></tr></tbody></table>"
      ],
      "text/plain": [
       "10×6 DataFrames.DataFrame. Omitted printing of 2 columns\n",
       "│ Row │ loglikelihood_Newton │ loglikelihood_MM │ niters_Newton │ niters_MM │\n",
       "├─────┼──────────────────────┼──────────────────┼───────────────┼───────────┤\n",
       "│ 1   │ -37358.4             │ -37364.4         │ 7.0           │ 100.0     │\n",
       "│ 2   │ -42179.2             │ -42179.5         │ 5.0           │ 16.0      │\n",
       "│ 3   │ -39985.3             │ -39985.6         │ 6.0           │ 46.0      │\n",
       "│ 4   │ -40519.5             │ -40519.9         │ 6.0           │ 64.0      │\n",
       "│ 5   │ -43488.8             │ -43488.9         │ 7.0           │ 7.0       │\n",
       "│ 6   │ -41191.3             │ -41191.5         │ 6.0           │ 7.0       │\n",
       "│ 7   │ -37702.5             │ -37703.0         │ 6.0           │ 71.0      │\n",
       "│ 8   │ -40304.0             │ -40304.3         │ 5.0           │ 47.0      │\n",
       "│ 9   │ -43130.8             │ -43131.3         │ 6.0           │ 52.0      │\n",
       "│ 10  │ -43709.7             │ -43709.8         │ 4.0           │ 6.0       │"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = DataFrame(loglikelihood_Newton = logl_nt, \n",
    "    loglikelihood_MM = logl_mm, \n",
    "    niters_Newton = niters_nt, \n",
    "    niters_MM = niters_mm, \n",
    "    time_Newton = time_nt, \n",
    "    time_MM = time_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, both Netwon's method and the MM method provide similar results. And we can see that the Newton's method takes less iterations compared with the MM method and it's more efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $Q$ function can be written as:   \n",
    "\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$  \n",
    "\n",
    "Since $\\ln\\Gamma(x)$ is a convex function, we can have:  \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ln\\Gamma(|\\alpha|) \\geq \\ln\\Gamma(|\\alpha|^t) \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
