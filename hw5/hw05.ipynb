{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 5\n",
    "\n",
    "**Due June 15 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. In [HW4](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html), we worked out a Newton's method. In this homework, we explore the MM and EM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Q1  \n",
    "\n",
    "In HW4, we have already shown that the log-likelihood of the Dirichlet-multinomial distribution is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\alpha) &= \n",
    "\\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)] \\\\ &= \n",
    "\\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\ln \\frac{\\Gamma(\\alpha_j + x_{ij})}{\\Gamma(\\alpha_j)} -  \\sum_{i=1}^n \\ln \\frac{\\Gamma(|\\alpha|+|\\mathbf{x}_i|)}{\\Gamma(|\\alpha|)} \\\\ &=\n",
    "\\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d \\ln[\\alpha_j(\\alpha_j+1)\\cdots(\\alpha_j+x_{ij}-1)] - \n",
    "\\sum_{i=1}^n \\ln[|\\alpha|(|\\alpha|+1)\\cdots(|\\alpha|+|\\mathbf{x}_i|-1)] \\\\ &=\n",
    "\\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Q2  \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\prod_{j=1}^d \\Gamma(\\alpha_j)}{\\Gamma(|\\alpha|)} = \\int_{\\Delta_d} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,d\\mathbf{p}\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "We can take derivative of $\\alpha_i$ from both sides of the equation:  \n",
    "\n",
    "$$\n",
    "\\frac{\\Gamma(|\\alpha|)\\prod_{j=1, j \\neq i}^d \\Gamma(\\alpha_j)\\Gamma'(\\alpha_i)}{\\Gamma^2(|\\alpha|)} =\n",
    "\\int_{\\Delta_d}\\ln(p_i) \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,d\\mathbf{p}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\prod_{j=1}^d\\Gamma(\\alpha_j)}{\\Gamma(|\\alpha|)}(\\frac{\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)} -\n",
    "\\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}) =\n",
    "\\int_{\\Delta_d}\\ln(p_i) \\prod_{j=1}^d p_j^{\\alpha_j-1} \\,d\\mathbf{p}\n",
    "$$  \n",
    "\n",
    "Then we can have   \n",
    "\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Q3  \n",
    "\n",
    "If we treat the unobserved multinomial parameters as hidden variables, the density of complete data can be expressed as $f(\\mathbf{p},\\mathbf{x}| \\alpha)$  \n",
    "\n",
    "Then the conditional expectation:  \n",
    "\n",
    "$$\n",
    "Q(\\alpha|\\alpha^{(t)}) = E_{\\mathbf{p}|x,\\alpha^{(t)}}[\\ln f(\\mathbf{p},\\mathbf{x}\\,|\\, \\alpha)\\,|\\,x, \\alpha^{(t)})]\n",
    "$$  \n",
    "\n",
    "Then we need to compute $\\ln f(\\mathbf{p},\\mathbf{x}\\,|\\, \\alpha)$. The density of Dirichlet-multinomial distribution is \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\pi(\\mathbf{p}) =  \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1},\n",
    "\\end{eqnarray*} \n",
    "$$\n",
    "where $|\\alpha|=\\sum_{j=1}^d \\alpha_j$.  \n",
    "\n",
    "$$\n",
    "f(x|\\mathbf{p}, \\alpha) = \\frac{\\Gamma(\\sum_{j=1}^d(x_j+1))}{\\prod_{j=1}^d\\Gamma(x_j+1)}\\prod_{j=1}^dp_j^{x_j}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "f(x,\\mathbf{p}\\,|\\,\\alpha) = \\frac{\\Gamma(\\sum_{j=1}^d(x_j+1))}{\\prod_{j=1}^d\\Gamma(x_j+1)}\n",
    "\\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d\\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{x_j+\\alpha_j-1}\n",
    "$$  \n",
    "\n",
    "$$\n",
    "f(x,\\mathbf{p}\\,|\\,\\alpha) = \\ln\\Gamma(\\sum_{j=1}^d(x_j+1)) + \\ln\\Gamma(|\\alpha|) -\n",
    "\\sum_{j=1}^d\\Gamma(x_j+1) - \\sum_{j=1}^d\\Gamma(\\alpha_j) + \n",
    "\\sum_{j=1}^d(\\alpha_j+x_j-1)\\ln p_j\n",
    "$$  \n",
    "\n",
    "The Q function can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Q(\\alpha|\\alpha^{(t)}) &= \\sum_{i=1}^n\\sum_{j=1}^d(\\alpha_j+x_j-1)E(\\ln p_j) - \n",
    "n\\sum_{j=1}^d\\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{t} \\\\\n",
    "&= \\sum_{i=1}^n\\sum_{j=1}^d[\\Psi(\\alpha_j) - \\Psi(|\\alpha|)](\\alpha_j+x_j-1)E(\\ln p_j)- \n",
    "n\\sum_{j=1}^d\\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{t} \\\\\n",
    "&= \\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Q4  \n",
    "\n",
    "In order to show it's a minorizing function to $L(\\alpha)$, we need to prove:  \n",
    "\n",
    "$$\n",
    " g(\\alpha|\\alpha^{(t)}) \\leq L(\\alpha) \\\\\n",
    " g(\\alpha^{(t)}|\\alpha^{(t)}) == L(\\alpha^{(t)})\n",
    "$$  \n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$  \n",
    "\n",
    "\n",
    "The $-\\ln(\\alpha_j) + k$ is a convex, non-increasing function. According to Jensen inequality, we can have:  \n",
    "\n",
    "$$\n",
    "-\\ln(\\alpha_j + k) \\leq -\\ln(\\alpha_j\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)} + k} + k\\frac{k}{\\alpha_j^{(t)}+k})\n",
    "\\leq -\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\ln(\\alpha_j)\n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\ln(\\alpha_j) \\leq \\ln(\\alpha_j + k) \n",
    "$$  \n",
    "\n",
    "Since $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, we can take summation on both sides:  \n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j \\leq \n",
    "\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) \n",
    "$$\n",
    "\n",
    "According to the hyperplane supporting inequality $f(x) + f'(x)(y-x) \\leq f(y)$, we let $y=|\\alpha|$, $x = |\\alpha^{(t)}|$, we can have:  \n",
    "\n",
    "$$\n",
    "-\\frac{1}{|\\alpha^{(t)}| + k}|\\alpha| + c^{(t)} \\leq -\\ln(|\\alpha|+k)\n",
    "$$  \n",
    "\n",
    "Since $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, we can take summation at both sides:  \n",
    "\n",
    "$$\n",
    "- \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha|+ c^{(t)} \\leq - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)\n",
    "$$  \n",
    "\n",
    "The equality achieves when $\\alpha_j == \\alpha_j^{(t)}$  So we have shown that $g(\\alpha|\\alpha^{(t)})$ is the minorizing function to $L(\\alpha)$.  \n",
    "\n",
    "From the above, we know the minorizing function of $L(\\alpha)$ is $g(\\alpha|\\alpha^{(t)})$. In order to get the update for $\\alpha_j$, we can take the derivative of $g(\\alpha|\\alpha^{(t)})$ with respect to $\\alpha_j$ and get the $\\alpha_j^{(t+1)}$ by solving $\\frac{dg(\\alpha|\\alpha^{(t)})}{d\\alpha_j} = 0$:  \n",
    "\n",
    "$$\n",
    "- \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha_j^{(t)}|+k} + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\frac{1}{\\alpha_j}=0 \n",
    "$$  \n",
    "\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_logpdf"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code from HW4: compute the log likelihood of Dirichlet-multinomial distribution\n",
    "function dirmult_logpdf(x::Vector, α::Vector)\n",
    "    # your code here\n",
    "    sum_x = sum(x)\n",
    "    sum_α = sum(α)\n",
    "    L = lgamma(sum_x + 1) - sum(lgamma.(x + 1)) + \n",
    "        sum(lgamma.(x + α)) - sum(lgamma.(α)) - \n",
    "        lgamma(sum_α + sum_x) + lgamma(sum_α)\n",
    "    return L\n",
    "end\n",
    "\n",
    "function dirmult_logpdf!(r::Vector, X::Matrix, α::Vector)\n",
    "    for j in 1:size(X, 2)\n",
    "        r[j] = dirmult_logpdf(X[:, j], α)\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    dirmult_logpdf(X, α)\n",
    "    \n",
    "Compute the log-pdf of Dirichlet-multinomial distribution with parameter `α` \n",
    "at each data point in `X`. Each column of `X` is one data point.\n",
    "\"\"\"\n",
    "function dirmult_logpdf(X::Matrix, α::Vector)\n",
    "    r = zeros(size(X, 2))\n",
    "    dirmult_logpdf!(r, X, α)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "    X::AbstractMatrix; \n",
    "    #α0::Vector = nothing,\n",
    "    maxiters::Int = 100, \n",
    "    tolfun = 1e-6\n",
    "    )\n",
    "    tot_n = size(X, 2)\n",
    "    tot_d = size(X, 1)\n",
    "    α0 = nothing\n",
    "    nonzero_indx = find(sum(X, 2) .!= 0)\n",
    "    # rule zero zeros\n",
    "    X = X[nonzero_indx, :]\n",
    "    # Compute X_i\n",
    "    X_rowsum = sum(X, 1)\n",
    "    n = size(X, 2)\n",
    "    d = size(X, 1)\n",
    "    # If the α is no initialized, use the moment estimator as a starting point\n",
    "    if α0 == nothing\n",
    "        p_hat = X ./ X_rowsum\n",
    "        v = sum(sum(p_hat.^2, 2) ./ sum(p_hat, 2))\n",
    "        α0 = sum(p_hat, 2) / n * (d - v) / (v - 1)\n",
    "    end\n",
    "    α0_sum = sum(α0)\n",
    "    # compute max x_ij - 1\n",
    "    max_ij_1 = maximum(X) - 1\n",
    "    # compute max |x_i| - 1\n",
    "    max_abs_1 = maximum(X_rowsum) - 1\n",
    "    # compute the S matrix\n",
    "    S = zeros(size(X, 1), max_ij_1 + 1)\n",
    "    ks = Array(range(0, Int(max_ij_1 + 1)))\n",
    "    # preallocate memory\n",
    "    aks = zeros(d, Int(max_ij_1 + 1))\n",
    "    for i = 1:Int(max_ij_1 + 1)\n",
    "        S[:, i] = sum(X .> (i - 1), 2)\n",
    "    end\n",
    "    kr = range(0, Int(max_abs_1 + 1))\n",
    "    R = zeros(max_abs_1 + 1)\n",
    "    for i = 1:Int(max_abs_1 + 1)\n",
    "        R[i] = sum(X_rowsum .> (i - 1))\n",
    "    end\n",
    "    loglold = sum(dirmult_logpdf(X, squeeze(α0, 2)))\n",
    "    # your code here\n",
    "    niter = 0\n",
    "    err = 100\n",
    "    # preallocate memory\n",
    "    nu = zeros(d, 1)\n",
    "    α1 = zeros(d, 1)\n",
    "    while (niter < maxiters) && (err > tolfun)\n",
    "        aks .= broadcast(+, α0, ks')\n",
    "        nu .= sum(S ./ aks, 2)\n",
    "        de = sum(R ./ (kr .+ α0_sum))\n",
    "        # compute a_(t + 1)\n",
    "        α1 .= (nu ./ de) .* α0\n",
    "        # compute new loglikelihood\n",
    "        logl = sum(dirmult_logpdf(X, squeeze(α1, 2)))\n",
    "        err = abs(loglold - logl) / (abs(loglold) - 1)\n",
    "        α0 .= α1\n",
    "        α0_sum = sum(α0)\n",
    "        loglold = logl\n",
    "        niter = niter + 1\n",
    "    end\n",
    "    # output\n",
    "    # compute logl, gradient, Hessian from final iterate\n",
    "    c = - sum(trigamma.(broadcast(+, α0_sum, X_rowsum)) - \n",
    "            trigamma(α0_sum))\n",
    "    H_tempt = - Diagonal(squeeze(sum(broadcast(-, trigamma.(broadcast(+, X, α0)), \n",
    "        trigamma.(α0)), 2), 2)) - c * ones(d, d)\n",
    "    # compute observed information matrix (inverse of hessian)\n",
    "    obsinfo = zeros(tot_d, tot_d)\n",
    "    obsinfo[nonzero_indx, nonzero_indx] = - H_tempt\n",
    "    # compute gradients\n",
    "    dL = sum(broadcast(-, \n",
    "        digamma.(broadcast(+, X, α0)), digamma.(α0)), 2) - \n",
    "        sum(digamma.(broadcast(+, α0_sum, X_rowsum)) - digamma(α0_sum))\n",
    "    ∇ = zeros(tot_d)\n",
    "    fill!(∇, sum(digamma.(broadcast(+, α0_sum, X_rowsum)) - digamma(α0_sum)))\n",
    "    ∇[nonzero_indx] = dL\n",
    "    α = zeros(tot_d)\n",
    "    α[nonzero_indx] = α0\n",
    "    logl = loglold\n",
    "    return logl, niter, α, ∇, obsinfo\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-477833.5711360321"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_n = size(X, 2)\n",
    "tot_d = size(X, 1)\n",
    "α0 = nothing\n",
    "nonzero_indx = find(sum(X, 2) .!= 0)\n",
    "# rule zero zeros\n",
    "X = X[nonzero_indx, :]\n",
    "X_rowsum = sum(X, 1)\n",
    "niter = 0\n",
    "n = size(X, 2)\n",
    "d = size(X, 1)\n",
    "max_ij_1 = maximum(X) - 1\n",
    "# compute max |x_i| - 1\n",
    "max_abs_1 = maximum(X_rowsum) - 1\n",
    "p_hat = X ./ X_rowsum\n",
    "v = sum(sum(p_hat.^2, 2) ./ sum(p_hat, 2))\n",
    "α0 = sum(p_hat, 2) / n * (d - v) / (v - 1)\n",
    "α0_sum = sum(α0)\n",
    "S = zeros(size(X, 1), max_ij_1 + 1);\n",
    "ks = Array(range(0, Int(max_ij_1 + 1)));\n",
    "for i = 1:Int(max_ij_1 + 1)\n",
    "    S[:, i] = sum(X .> (i - 1), 2)\n",
    "end\n",
    "kr = range(0, Int(max_abs_1 + 1))\n",
    "R = zeros(max_abs_1 + 1)\n",
    "for i = 1:Int(max_abs_1 + 1)\n",
    "    R[i] = sum(X_rowsum .> (i - 1))\n",
    "end\n",
    "α1 = zeros(d, 1)\n",
    "aks = zeros(d, Int(max_ij_1 + 1));\n",
    "loglold = sum(dirmult_logpdf(X, squeeze(α0, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0e-5"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxiters = 100\n",
    "err = 100\n",
    "tolfun = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62×1 Array{Float64,2}:\n",
       "  8958.56\n",
       "  6267.87\n",
       "  6681.16\n",
       "  6636.33\n",
       "  5682.68\n",
       "  4134.7 \n",
       "  3603.49\n",
       "  6724.07\n",
       "  5743.63\n",
       "  6434.83\n",
       "  6772.14\n",
       "  6647.49\n",
       "  6101.55\n",
       "     ⋮   \n",
       "  6507.8 \n",
       "  6171.89\n",
       "  5202.63\n",
       "  6177.07\n",
       " 18087.8 \n",
       "  7798.21\n",
       "  6144.01\n",
       "  6660.0 \n",
       "  6524.49\n",
       "  5854.47\n",
       "  4513.69\n",
       "  4753.25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aks .= broadcast(+, α0, ks')\n",
    "nu = sum(S ./ aks, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005313891132398897"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de = sum(R ./ (kr .+ α0_sum))\n",
    "α1 .= (nu ./ de) .* α0\n",
    "logl = sum(dirmult_logpdf(X, squeeze(α1, 2)))\n",
    "err = abs(loglold - logl) / (abs(loglold) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2018spring/hw/hw4/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = Matrix{Float64}(CSV.read(\"optdigits.tra\", datarow = 1));\n",
    "X = digits[:, 1:64]';\n",
    "Y = digits[:, 65];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.722809 seconds (1.06 M allocations: 465.901 MiB, 4.54% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-472075.97234067833, 34, [0.0, 0.0809119, 0.895352, 2.10398, 2.01955, 0.764498, 0.14637, 0.0151384, 0.000389504, 0.301422  …  0.459779, 0.0272398, 0.000129783, 0.0654391, 0.92025, 2.11339, 1.94244, 0.945567, 0.231583, 0.0277856], [7708.64, -3.6524, -6.30488, -7.42404, -7.40155, -6.339, -4.30756, -3.55406, -3.46564, -4.76542  …  -5.7731, -3.57482, -3.46408, -3.66007, -6.46243, -7.47838, -7.55686, -6.89602, -4.75066, -3.5959], [0.0 0.0 … 0.0 0.0; 0.0 -90807.1 … 68.8037 68.8037; … ; 0.0 68.8037 … -25214.6 68.8037; 0.0 68.8037 … 68.8037 -2.68228e5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time dirmult_mm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
